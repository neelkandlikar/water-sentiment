{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataframe of search results imported with 141901 unique entries.\n",
      "Beginning of song to retrieve abstracts:\n",
      "Retrieving abstracts 0 to 10000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'json_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-015f0fa4f0aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://api.elsevier.com/content/abstract/scopus_id/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'?view=META&apikey='\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mabstracts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Chunk '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPanel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"module 'pandas' has no attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'json_normalize'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import xmltodict as saveme\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "#pip install xmltodict\n",
    "\n",
    "\n",
    "\n",
    "# Get the working and data directory paths\n",
    "wkdir = os.path.dirname(os.getcwd())\n",
    "data_dir  = wkdir + '/data'\n",
    "\n",
    "#Load in the SCOPUS WOS Search Results\n",
    "search_results = pd.read_csv(data_dir+'/scopus_search_results_r4.csv', low_memory = False)\n",
    "\n",
    "# !! Import list of API Keys\n",
    "api_keys = open(wkdir+'/code/keys.gitignore', \"r\").read().splitlines()\n",
    "\n",
    "\n",
    "print('Master dataframe of search results imported with '+str(len(search_results['eid'].unique()))+' unique entries.')\n",
    "\n",
    "#split list into 19 chunks:\n",
    "\n",
    "chunks = range(8713, int(8e4)+8713, 2)\n",
    "\n",
    "print('Beginning of song to retrieve abstracts:')\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Retrieving abstracts ' +str(i+8713)+ ' to ' +str(str(i+int(1e4)+8713)))\n",
    "    #set the api_key for this chunk\n",
    "    api_key = api_keys[i]\n",
    "    \n",
    "    #make an empty list to store failures\n",
    "    failed_pages = pd.DataFrame()\n",
    "\n",
    "    # create master dataframe with all the first article in the list\n",
    "\n",
    "    idx = search_results.iloc[chunk]['dc:identifier'].split(':')[1]\n",
    "    response = requests.get('https://api.elsevier.com/content/abstract/scopus_id/'+idx+'?view=META&apikey='+ api_key)\n",
    "\n",
    "    abstracts = pd.json_normalize(saveme.parse(response.text))\n",
    "\n",
    "    for j, a in tqdm(search_results[chunk+1:chunk+int(1e4)].iterrows(), desc='Chunk '+str(i)):\n",
    "        try:\n",
    "            #print(\"Working on article \" + str(i) +' of'  + str(len(search_results)))\n",
    "            #Get scupis ID:\n",
    "            idx = a['dc:identifier'].split(':')[1]\n",
    "\n",
    "            #Format URL to retrieve the abstract:\n",
    "            response = requests.get('https://api.elsevier.com/content/abstract/scopus_id/'+idx+'?view=META_ABS&apikey='+ api_key)\n",
    "            abstract = pd.json_normalize(saveme.parse(response.text))\n",
    "            abstracts = abstracts.append(abstract)\n",
    "            \n",
    "            time.sleep(0.12)\n",
    "            \n",
    "        except:\n",
    "            failed_pages[j] = idx\n",
    "            continue\n",
    "            \n",
    "    abstracts.to_csv(wkdir+'/data/abstracts_06192020_chunk_'+str(i+11)+'.csv')\n",
    "    \n",
    "    if len(failed_pages) > 0:\n",
    "        failed_pages.to_csv(wkdir+'/data/failed_queries_chunk'+str(i+11)+'.csv')\n",
    "        print('Failed abstract retrievals (' + str(len(failed_pages))+') saved to '+ data_dir)\n",
    "    else:\n",
    "        print('Zero failed retrievals.')\n",
    "        continue\n",
    "    \n",
    "    time.sleep(0.12)\n",
    "        \n",
    "    print('Done with chunk '+str(i+12))\n",
    "\n",
    "print(\"Ding, end of song!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check results that we already have\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali = pd.DataFrame({'dates': dates, 'calis': cals})\n",
    "# cali = cali.set_index('dates')\n",
    "\n",
    "# cali.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grps = pd.DataFrame(search_results.eid.groupby(search_results.index.year).count())\n",
    "\n",
    "# grps['calis'] = yrs['calis']\n",
    "\n",
    "# ax = grps.plot(x=grps.index, kind='bar')\n",
    "\n",
    "# # Make most of the ticklabels empty so the labels don't get too crowded\n",
    "# ticklabels = ['']*len(search_results.index)\n",
    "# # Every 4th ticklable shows the month and day\n",
    "# #ticklabels[::4] = [item.strftime('%b %d') for item in search_results.index[::4]]\n",
    "# # Every 12th ticklabel includes the year\n",
    "# ticklabels[::12] = [item.strftime('%b %d\\n%Y') for item in search_results.index[::12]]\n",
    "# ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))\n",
    "# ax.set_ylabel('article count')\n",
    "# ax.set_title('\"ground water\" OR groundwater OR ground-water OR aquifer')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('article_counts.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
